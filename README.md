# Police-Report
For the data pipeline, we first extracted data from our data sources. After importing relevant libraries, we made a bulk API call with our San Francisco Government API that returned the entire API JSON response. We then converted the API JSON response into a DataFrame that was set to the variable "df". For our second data source, we simply ingested the Rhode Island Kaggle CSV dataset and created another DataFrame that was set to the variable "df2". Next, we cleaned and transformed the data. We did data cleaning for the San Francisco API DataFrame by filling null values for certain columns, dropping any additional null values that couldn't be filled, dropping four columns, and removing outliers. We did data transformation for the San Francisco API DataFrame by combining the latitude and longitude columns into one single column named coordinates. We also changed the data types of six columns to match them with the original dataset on the San Francisco Government's website. For the Rhode Island Kaggle DataFrame, we did data cleaning by dropping two columns, dropping any additional null values, and removing outliers. We did data transformation for the Rhode Island Kaggle DataFrame by changing the data types of three columns so that they would be easier to do analysis and visualizations with. We attempted to load both cleaned DataFrames as CSVs into a SQL Database using SQLAlchemy, but unfortunately, we were unsuccessful. The code for loading the cleaned DataFrames is included in the "DataWPythonFinalProject.ipynb" file as well as the Python script file "ToDataBase.py". The "DataWPythonFinalProject.ipynb" has included all necessary code, comments, and headings that outline our entire data pipeline process in detail. The Python script file "ToDataBase.py" can be run in a terminal using the command "python ToDataBase.py" to try to load both cleaned DataFrames CSVs into a SQL Database using SQLAlchemy.
