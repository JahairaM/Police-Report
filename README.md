# Police-Report
Our data pipeline follows a structured approach consisting of three main stages: data extraction, data cleaning and transformation, and data loading. We extracted data from two sources: the San Francisco Government API and the Rhode Island Kaggle dataset. The API provided a bulk JSON response, which we converted into a Pandas DataFrame (df). Meanwhile, we ingested the Rhode Island dataset as a CSV file and stored it in another DataFrame (df2).

To ensure data quality, we performed cleaning and transformation on both datasets. For the San Francisco API data, we filled missing values in key columns, dropped remaining null values, removed four unnecessary columns, and filtered out outliers. Additionally, we combined the latitude and longitude columns into a single coordinates column and converted six columns to match the data types used in the original dataset from the San Francisco Governmentâ€™s website. Similarly, for the Rhode Island Kaggle data, we dropped two unnecessary columns, handled missing values, and removed outliers. We also adjusted the data types of three columns to improve compatibility for analysis and visualization.

Finally, we attempted to load the cleaned DataFrames into a SQL database using SQLAlchemy, but this step was unsuccessful. The code for this process is included in two key files: "DataWPythonFinalProject.ipynb", which contains the entire data pipeline with comments and headings, and "ToDataBase.py", a standalone script that can be executed using the command python ToDataBase.py to retry loading the cleaned DataFrames into a SQL database.

